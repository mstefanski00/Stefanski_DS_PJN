{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b6c4ef",
   "metadata": {},
   "source": [
    "# PJN - Lab 9 RAG Advanced\n",
    "\n",
    "#### Mikołaj Stefański\n",
    "\n",
    "## ETAP I — dynamiczny retrieval adaptacyjny\n",
    "\n",
    "### Zadanie 1 – analiza zapytania i zmiana wagi hybrydy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc14530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZAPYTANIE                                | TRYB       | ES WAGA  | QDRANT  \n",
      "--------------------------------------------------------------------------------\n",
      "Jak poprawić pracę zespołową?            | SEMANTIC   | 0.4      | 0.6     \n",
      "Co zawiera dokument PAN LP-78?           | FACTUAL    | 0.8      | 0.2     \n",
      "Czy inflacja 2022 była wyższa?           | FACTUAL    | 0.8      | 0.2     \n",
      "Co Sinek mówił o liderach?               | SEMANTIC   | 0.4      | 0.6     \n",
      "Wydarzenia klimatyczne w Polsce          | SEMANTIC   | 0.4      | 0.6     \n",
      "Protokół HTTP błędy 404                  | FACTUAL    | 0.8      | 0.2     \n",
      "Definicja empatii                        | SEMANTIC   | 0.4      | 0.6     \n",
      "Raport ONZ 2023                          | FACTUAL    | 0.8      | 0.2     \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "from rag.reasoning.query_analysis import analyze_query_intent\n",
    "from rag.retrieval.search_engine import hybrid_search\n",
    "\n",
    "test_queries = [\n",
    "    \"Jak poprawić pracę zespołową?\",          \n",
    "    \"Co zawiera dokument PAN LP-78?\",          \n",
    "    \"Czy inflacja 2022 była wyższa?\",          \n",
    "    \"Co Sinek mówił o liderach?\",             \n",
    "    \"Wydarzenia klimatyczne w Polsce\",         \n",
    "    \"Protokół HTTP błędy 404\",                 \n",
    "    \"Definicja empatii\",                       \n",
    "    \"Raport ONZ 2023\",                        \n",
    "]\n",
    "\n",
    "print(f\"{'ZAPYTANIE':<40} | {'TRYB':<10} | {'ES WAGA':<8} | {'QDRANT':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for query in test_queries:\n",
    "    config = analyze_query_intent(query)\n",
    "    \n",
    "    try:\n",
    "        q_res, es_res, hybrid_res = hybrid_search(\n",
    "            query, \n",
    "            limit=3, \n",
    "            weight_es=config['es_weight'], \n",
    "            weight_qdrant=config['qdrant_weight']\n",
    "        )\n",
    "        \n",
    "        print(f\"{query:<40} | {config['mode'].upper():<10} | {config['es_weight']:<8} | {config['qdrant_weight']:<8}\")\n",
    "        \n",
    "        if hybrid_res:\n",
    "            top_text = hybrid_res[0]['text'][:50].replace('\\n', ' ')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Błąd przy zapytaniu '{query}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f751d8e3",
   "metadata": {},
   "source": [
    "#### Wyniki\n",
    "\n",
    "System poprawnie rozpoznaje typ zapytania. Pytania z akronimami czy liczbami klasyfikuje jako `FACTUAL`, a pytania abstrakcyjne jako `SEMANTIC`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e081ac",
   "metadata": {},
   "source": [
    "### Zadanie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb0c35cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test filtrowania dla zapytania: 'PAN'.\n",
      "\n",
      "Surowe dane z bazy: 10\n",
      "\n",
      "Statystyki filtra:\n",
      " -> Odrzucono - za krótkie: 7\n",
      " -> Współczynnik odrzuceń: 70.0%\n",
      " -> Wynik końcowy: 3 dokumentów\n",
      "\n",
      "Odfiltrowane dokumenty:\n",
      " 1. [47 słów] Wśród laureatów znajdują się m.in. prof. dr hab. Barbara Bilińska, członek kores.\n",
      " 2. [35 słów] I powiedział pewien człowiek wierzący, z rodu Faraona, który ukrywał swoją wiarę.\n",
      " 3. [47 słów] Wśród laureatów znajdują się m.in. prof. dr hab. Barbara Bilińska, członek kores.\n"
     ]
    }
   ],
   "source": [
    "from rag.reasoning.filtering import filter_retrieved_docs\n",
    "from rag.retrieval.search_engine import hybrid_search\n",
    "\n",
    "query = \"PAN\" \n",
    "\n",
    "print(f\"Test filtrowania dla zapytania: '{query}'.\\n\")\n",
    "\n",
    "_, _, raw_results = hybrid_search(query, limit=10)\n",
    "\n",
    "print(f\"Surowe dane z bazy: {len(raw_results)}\")\n",
    "\n",
    "clean_docs, stats = filter_retrieved_docs(raw_results, min_words=30, max_docs=3)\n",
    "\n",
    "print(\"\\nStatystyki filtra:\")\n",
    "print(f\" -> Odrzucono - za krótkie: {stats['rejected_short']}\")\n",
    "print(f\" -> Współczynnik odrzuceń: {stats['rejected_ratio']}\")\n",
    "print(f\" -> Wynik końcowy: {stats['final_count']} dokumentów\")\n",
    "\n",
    "print(\"\\nOdfiltrowane dokumenty:\")\n",
    "for i, doc in enumerate(clean_docs, 1):\n",
    "    preview = doc['text'][:80].replace('\\n', ' ')\n",
    "    print(f\" {i}. [{len(doc['text'].split())} słów] {preview}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ab487",
   "metadata": {},
   "source": [
    "#### Wyniki\n",
    "\n",
    "Odrzucenie 7 tekstów oszczędza czas i zasoby potrzebne do ich przetworzenia przez LLM. Model nie będzie próbował zgadnąć odpowiedzi na podstawie urwanego nagłówka czy stopki zawierającej słowo 'PAN'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef1ad6",
   "metadata": {},
   "source": [
    "## Etap II — Query Decomposition + Query Rewriting\n",
    "\n",
    "### Zadanie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b856b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam test dekompozycji.\n",
      "\n",
      "Zapytanie główne: 'Jak poprawić pracę zespołową?'\n",
      " -> Podpytania wygenerowane przez AI:\n",
      "    * Jakie są kluczowe elementy dobrej pracy w zespole?\n",
      "    * Co wpływa na efektywność zespołu?\n",
      "============================================================\n",
      "Zapytanie główne: 'Jakie dane o klimacie po 2020 w Polsce?'\n",
      " -> Podpytania wygenerowane przez AI:\n",
      "    * Czy istnieje dostępna publiczna baza danych z danymi klimatycznymi dla Polski po 2020 roku?\n",
      "    * Jakie konkretne dane (np. temperatura, wilgotność, opady) interesują Cię najbardziej?\n",
      "============================================================\n",
      "Zapytanie główne: 'Co Sinek mówił o liderach?'\n",
      " -> Podpytania wygenerowane przez AI:\n",
      "    * Jakie informacje na temat liderów podał Sinek?\n",
      "    * W jakim kontekście Sinek omawiał liderów?\n",
      "============================================================\n",
      "Zapytanie główne: 'Czy inflacja rośnie?'\n",
      " -> Podpytania wygenerowane przez AI:\n",
      "    * Jaka jest aktualna inflacja w Polsce?\n",
      "    * Jakie są trendy inflacji w ostatnich miesiącach? \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from rag.reasoning.query_analysis import decompose_query, generate_clarification_question\n",
    "\n",
    "complex_queries = [\n",
    "    \"Jak poprawić pracę zespołową?\",           \n",
    "    \"Jakie dane o klimacie po 2020 w Polsce?\",\n",
    "    \"Co Sinek mówił o liderach?\",              \n",
    "    \"Czy inflacja rośnie?\"                    \n",
    "    ]\n",
    "\n",
    "print(\"Rozpoczynam test dekompozycji.\\n\")\n",
    "\n",
    "for q in complex_queries:\n",
    "    print(f\"Zapytanie główne: '{q}'\")\n",
    "    \n",
    "    result = decompose_query(q)\n",
    "    \n",
    "    print(\" -> Podpytania wygenerowane przez AI:\")\n",
    "    for sub_q in result.get('sub_questions', []):\n",
    "        print(f\"    * {sub_q}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12034132",
   "metadata": {},
   "source": [
    "#### Wyniki\n",
    "\n",
    "Model Gemma dobrze poradził sobie z dekompozycją. Zamiast odpowiadać tak lub nie, pyta o \"aktualną inflację\" i \"trendy\". To bardzo dobry punkt wyjścia dla naszego systemu, bo pozwala pobrać konkretne dane liczbowe, natomiast rozbicie na \"kluczowe elementy\" i \"efektywność\" pozwoli systemowi wyszukać szerszy kontekst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa644c29",
   "metadata": {},
   "source": [
    "### Zadanie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b32d837e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam test pytań doprecyzowujących.\n",
      "\n",
      "Zapytanie: 'Co mówi PAN o kryzysie?'\n",
      " -> Czy jest niejednoznaczne? TAK\n",
      " -> Proponowane interpretacje:\n",
      "    * Co Pan profesor z Polskiej Akademii Nauk powiedział o kryzysie?\n",
      "    * Czy PAN komentował kryzys w jakimś wypowiedzi?\n",
      "============================================================\n",
      "Zapytanie: 'Jaki ma sens odpowiedzialność?'\n",
      " -> Czy jest niejednoznaczne? TAK\n",
      " -> Proponowane interpretacje:\n",
      "    * Jaki ma sens **wymiar odpowiedzialności**?\n",
      "    * Jaki ma sens **poziom odpowiedzialności w danej sytuacji**?\n",
      "============================================================\n",
      "Zapytanie: 'Jak zarządzać ludźmi?'\n",
      " -> Czy jest niejednoznaczne? TAK\n",
      " -> Proponowane interpretacje:\n",
      "    * **Interpretacja 1:** Jak zarządzać ludźmi w kontekście zarządzania zespołem, jak to zrobić w pracy, by zespół był efektywny i produktywny?\n",
      "    * **Interpretacja 2:** Jak zarządzać ludźmi w kontekście relacji interpersonalnych, jak budować zdrowe i szczęśliwe relacje z innymi ludźmi?\n",
      "============================================================\n",
      "Zapytanie: 'Zamek w Polsce'\n",
      " -> Czy jest niejednoznaczne? TAK\n",
      " -> Proponowane interpretacje:\n",
      "    * Zamek w Polsce jako konkretny zamek (np. Zamek Kórnik) lub typ zamków w Polsce.\n",
      "    * Zamek w Polsce jako ogólny temat związane z historią i architekturą zamków w Polsce.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from rag.reasoning.query_analysis import generate_clarification_question\n",
    "\n",
    "ambiguous_queries = [\n",
    "    \"Co mówi PAN o kryzysie?\",         \n",
    "    \"Jaki ma sens odpowiedzialność?\",  \n",
    "    \"Jak zarządzać ludźmi?\",           \n",
    "    \"Zamek w Polsce\"                   \n",
    "]\n",
    "\n",
    "print(\"Rozpoczynam test pytań doprecyzowujących.\\n\")\n",
    "\n",
    "for q in ambiguous_queries:\n",
    "    print(f\"Zapytanie: '{q}'\")\n",
    "    \n",
    "    result = generate_clarification_question(q)\n",
    "    \n",
    "    is_ambiguous = result.get('is_ambiguous', False)\n",
    "    print(f\" -> Czy jest niejednoznaczne? {'TAK' if is_ambiguous else 'NIE'}\")\n",
    "    \n",
    "    if is_ambiguous:\n",
    "        print(\" -> Proponowane interpretacje:\")\n",
    "        for interp in result.get('interpretations', []):\n",
    "            print(f\"    * {interp}\")\n",
    "            \n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7f504",
   "metadata": {},
   "source": [
    "#### Wyniki\n",
    "\n",
    "1. Co mówi PAN o kryzysie?\n",
    "\n",
    "    Model słusznie powiązał akronim \"PAN\" z Polską Akademią Nauk, odróżniając go od zwrotu grzecznościowego \"Pan\". ES zwróciłby zarówno dokumenty o naukowcach, jak i cytaty z Biblii (\"Pan Bóg\"). Dzięki klaryfikacji system wie, że użytkownik szuka opinii eksperckiej/instytucjonalnej.\n",
    "\n",
    "2. Jak zarządzać ludźmi?\n",
    "\n",
    "    Model doskonale rozdzielił kontekst zawodowy od interpersonalnego. To dwa rozdzielne zbiory dokumentów. Bez tego pytania RAG mógłby mieszać porady biznesowe z poradnikami psychologicznymi, tworząc niespójną odpowiedź.\n",
    "\n",
    "3. Zamek w Polsce.\n",
    "\n",
    "    Rozróżnienie między konkretną encją, a tematem ogólnym. Pozwala systemowi zdecydować, czy szukać listy zamków, czy definicji/historii.\n",
    "\n",
    "4. Jaki ma sens odpowiedzialność?\n",
    "\n",
    "    Interpretacje są dość abstrakcyjne. Zabrakło wyraźnego rozróżnienia np. na odpowiedzialność prawną vs moralną. Mniejsza niż w pozostałych, ale nadal sygnalizuje użytkownikowi, że pytanie jest zbyt ogólne.\n",
    "\n",
    "\n",
    "\n",
    "| Zapytanie                      | Przydatność                          |\n",
    "| ------------------------------ | ------------------------------------ |\n",
    "| Co mówi PAN o kryzysie?        | Duża                                 |\n",
    "| Jak zarządzać ludźmi?          | Bardzo duża                          |\n",
    "| Zamek w Polsce.                | Duża                                 |\n",
    "| Jaki ma sens odpowiedzialność? | Średnia                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c01237",
   "metadata": {},
   "source": [
    "## Etap III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8928a052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam test pamięci systemowej.\n",
      "\n",
      "Zapytanie: 'Jaki jest kod błędu X-99 w reaktorze?'\n",
      "Odłożono pytanie do wyjaśnienia: 'Jaki jest kod błędu X-99 w reaktorze?'\n",
      "\n",
      "Zawartość pliku pending.json:\n",
      " [ID: 1] Status: PENDING | Pytanie: Jaki jest kod błędu X-99 w reaktorze?\n"
     ]
    }
   ],
   "source": [
    "from rag.memory.memory_manager import memory\n",
    "\n",
    "trudne_pytanie = \"Jaki jest kod błędu X-99 w reaktorze?\"\n",
    "\n",
    "print(f\"Rozpoczynam test pamięci systemowej.\\n\")\n",
    "print(f\"Zapytanie: '{trudne_pytanie}'\")\n",
    "\n",
    "memory.add_pending_query(trudne_pytanie, reason=\"brak dokumentów w bazie\")\n",
    "\n",
    "pending_list = memory.get_pending_queries()\n",
    "\n",
    "print(\"\\nZawartość pliku pending.json:\")\n",
    "for item in pending_list:\n",
    "    print(f\" [ID: {item['id']}] Status: {item['status'].upper()} | Pytanie: {item['query']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f9e8c0",
   "metadata": {},
   "source": [
    "#### Wyniki\n",
    "\n",
    "Plik pending.json został utworzony, a system poprawnie zapisał pytanie, na które nie znalazł odpowiedzi. Mechanizm pamięci działa. Zatem został zbudowany fundament, żeby RAG nie musiał wymyślać odpowiedzi, gdy czegoś nie wie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb9f1e",
   "metadata": {},
   "source": [
    "## Etap IV — warstwa weryfikacji\n",
    "\n",
    "### Zadanie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "165601eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam test walidatora.\n",
      "\n",
      "Test 1: Pytanie: 'Czym jest praca zespołowa?'\n",
      " -> Czy poprawne? True\n",
      " -> Wyjaśnienie: Odpowiedź systemu zawiera poprawne definicje pracy zespołowej. W źródłach wskazano, że praca zespołowa to działanie grupy osób w celu osiągnięcia wspólnego celu. Takie definicje są zgodne z zasadami pracy zespołowej.\n",
      "============================================================\n",
      "Test 2: Pytanie: 'Jaki jest ulubiony kolor prezesa?'\n",
      " -> Czy poprawne? False\n",
      " -> Wyjaśnienie: Dostępne źródła nie dostarczają informacji o ulubionym kolorze prezesa.\n",
      "\n",
      "Wykryto halucynację. Uruchamiam Safe Mode.\n",
      "Odpowiedź dla użytkownika: Nie jestem pewien odpowiedzi na podstawie dostępnych dokumentów. Pytanie zostało zapisane do wyjaśnienia przez eksperta.\n"
     ]
    }
   ],
   "source": [
    "from rag.verification.validator import validate_rag_answer, safe_mode_retry\n",
    "\n",
    "print(\"Rozpoczynam test walidatora.\\n\")\n",
    "\n",
    "query_1 = \"Czym jest praca zespołowa?\"\n",
    "context_1 = [{\"text\": \"Praca zespołowa to skoordynowane działania grupy osób w celu osiągnięcia wspólnego celu.\"}]\n",
    "answer_1 = \"To działanie grupy ludzi dążących do wspólnego celu.\"\n",
    "\n",
    "print(f\"Test 1: Pytanie: '{query_1}'\")\n",
    "valid_1, reason_1 = validate_rag_answer(query_1, answer_1, context_1)\n",
    "print(f\" -> Czy poprawne? {valid_1}\")\n",
    "print(f\" -> Wyjaśnienie: {reason_1}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query_2 = \"Jaki jest ulubiony kolor prezesa?\"\n",
    "context_2 = [{\"text\": \"Prezes zarządza firmą i odpowiada za strategię finansową spółki.\"}] \n",
    "answer_2 = \"Ulubionym kolorem prezesa jest niebieski.\"\n",
    "\n",
    "print(f\"Test 2: Pytanie: '{query_2}'\")\n",
    "valid_2, reason_2 = validate_rag_answer(query_2, answer_2, context_2)\n",
    "\n",
    "print(f\" -> Czy poprawne? {valid_2}\") \n",
    "print(f\" -> Wyjaśnienie: {reason_2}\")\n",
    "\n",
    "if not valid_2:\n",
    "    print(\"\\nWykryto halucynację. Uruchamiam Safe Mode.\")\n",
    "    safe_response = safe_mode_retry(query_2, strategy=\"memory\")\n",
    "    print(f\"Odpowiedź dla użytkownika: {safe_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68351a47",
   "metadata": {},
   "source": [
    "#### Wyniki\n",
    "\n",
    "Logi wykazują, że walidator działa poprawnie:\n",
    "\n",
    "- Test 1: walidator zrozumiał kontekst, przeczytał definicję i potwierdził poprawność.\n",
    "- Test 2: walidator zauważył brak informacji w źródłach, odrzucił odpowiedź i uruchomił Safe Mode, który zapisał pytnie do pamięci."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c25a5",
   "metadata": {},
   "source": [
    "#### Pytanie: Jaki jest cel podobnego działania?\n",
    "\n",
    "1. Ochrona autorytetu: najgorsze co RAG może zrobić, to z pełnym przekonaniem podać nieprawdę. Safe Mode to hamulec bezpieczeństwa - lepiej żeby system powiedział, że nie znalazł informacji na ten temat, niż żeby zaczął zmyślać odpowiedź.\n",
    "\n",
    "2. Domykanie pętli: zapisanie pytania do pending.json pozwala człowiekowi zobaczyć, czego system nie wie. Można wtedy dodać brakujący dokument do bazy i następnym razem system już odpowie poprawnie.\n",
    "\n",
    "3. Autonomiczna naprawa: strategie takie jak modify_prompt pozwalają systemowi spróbować jeszcze raz, w inny sposób, zanim się podda. To symuluje ludzkie zachowanie – jak nie znajdzie się czegoś w internecie za pierwszym razem, zmieniamy słowa kluczowe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426cbcb",
   "metadata": {},
   "source": [
    "### Integracja systemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae73b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Test 1: Pytanie poprawne.\n",
      "================================================================================\n",
      "\n",
      "Zaczynamy! Pytanie użytkownika: 'Kim jest Barbara Bilińska?'\n",
      "Wykryto niejednoznaczność, ale kontynuuję na potrzeby testu.\n",
      "   -> System zapytałby: Kim jest Barbara Bilińska jako osoba publiczna?...\n",
      "Rozpoczynam analizę intencji. Tryb: SEMANTIC. Wagi -> ES: 0.4, Qdrant: 0.6\n",
      "Rozpoczynam filtrowanie. Wybrano: 4 dokumentów. Odrzucono krótkie: 0\n",
      "Generowanie odpowiedzi.\n",
      "   -> Wstępny draft: Prof. dr hab. Barbara Bilińska jest członkiem korespondent PAN....\n",
      "Weryfikacja faktów.\n",
      "Zweryfikowano pomyślnie.\n",
      "\n",
      "Finalna odpowiedź dla użytkownika:\n",
      "Prof. dr hab. Barbara Bilińska jest członkiem korespondent PAN.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Test 2: Celowa halucynacja.\n",
      "================================================================================\n",
      "\n",
      "Zaczynamy! Pytanie użytkownika: 'Podaj nazwę ulubionego koloru prezesa PAN.'\n",
      "Wykryto niejednoznaczność, ale kontynuuję na potrzeby testu.\n",
      "   -> System zapytałby: Nazwa ulubionego koloru prezesa PAN w kontekście jego osobistej preferencji barw...\n",
      "Rozpoczynam analizę intencji. Tryb: FACTUAL. Wagi -> ES: 0.8, Qdrant: 0.2\n",
      "Rozpoczynam filtrowanie. Wybrano: 4 dokumentów. Odrzucono krótkie: 0\n",
      "Generowanie odpowiedzi.\n",
      "   -> Wstępny draft: Nie wiem....\n",
      "Weryfikacja faktów.\n",
      "Halucynacja modelu! Odrzucono: System nie ma żadnych informacji na temat ulubionego koloru prezesa PAN.  W źródłach znajduje się jedynie informacja o członkowcach PAN.\n",
      "Odłożono pytanie do wyjaśnienia: 'Podaj nazwę ulubionego koloru prezesa PAN.'\n",
      "\n",
      "Finalna odpowiedź dla użytkownika:\n",
      "Nie jestem pewien odpowiedzi na podstawie dostępnych dokumentów. Pytanie zostało zapisane do wyjaśnienia przez eksperta.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rag_pipeline import run_rag_pipeline\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Test 1: Pytanie poprawne.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "query_a = \"Kim jest Barbara Bilińska?\"\n",
    "response_a = run_rag_pipeline(query_a)\n",
    "\n",
    "print(f\"\\nFinalna odpowiedź dla użytkownika:\\n{response_a}\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test 2: Celowa halucynacja.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "query_b = \"Podaj nazwę ulubionego koloru prezesa PAN.\"\n",
    "response_b = run_rag_pipeline(query_b)\n",
    "\n",
    "print(f\"\\nFinalna odpowiedź dla użytkownika:\\n{response_b}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed19754",
   "metadata": {},
   "source": [
    "#### Wyniki\n",
    "\n",
    "Przeprowadzone testy potwierdziły poprawność działania zaimplementowanego potoku RAG. System wykazał się odpornością na halucynacje oraz zdolnością do dynamicznego dostosowywania strategii wyszukiwania. Poniżej szczegółowa analiza kluczowych modułów:\n",
    "\n",
    "1. **Skuteczność warstwy weryfikacji**\n",
    "\n",
    "    Najważniejszym osiągnięciem jest skuteczna eliminacja halucynacji w teście nr 2. Standardowy RAG często zmyśla odpowiedź, próbując na siłę dopasować kontekst. Zaimplementowany moduł `validator` oparty na modelu gemma2:2b poprawnie zidentyfikował brak informacji o \"kolorze prezesa\" w dostarczonych dokumentach. Mechanizm ten działa jako krytyczny bezpiecznik, zmieniając system z generatora tekstu w wiarygodnego asystenta, który potrafi przyznać się do niewiedzy.\n",
    "\n",
    "2. **Dynamiczne sterowanie wyszukiwaniem**\n",
    "\n",
    "    System poprawnie rozpoznał intencje użytkownika, co widać w logach doboru wag:\n",
    "\n",
    "    - w przypadku pytania o definicję/osobę w teście: System zaklasyfikował intencję jako SEMANTIC (wagi: ES 0.4 / Qdrant 0.6), co pozwoliło na szersze, wektorowe dopasowanie kontekstu.\n",
    "\n",
    "    - w przypadku pytania o konkretny fakt w teście 2: system zmienił tryb na FACTUAL (wagi: ES 0.8 / Qdrant 0.2), priorytetyzując słowa kluczowe. Dowodzi to, że system nie traktuje każdego zapytania tak samo, lecz optymalizuje strategię retrievalu pod kątem specyfiki pytania.\n",
    "\n",
    "3. **Mechanizm \"Safe Mode\" i pamięć systemu**\n",
    "\n",
    "    Zamiast zwracać pusty błąd lub nieprawdziwą informację, system uruchomił procedurę Safe Mode. Zapisanie nierozwiązanego pytania do pending_queries.json stanowi fundament pod tzw. \"Human-in-the-loop\". W środowisku produkcyjnym pozwala to administratorom identyfikować luki w bazie wiedzy i je uzupełniać, co sprawia, że system uczy się na swoich porażkach.\n",
    "\n",
    "4. **Nadwrażliwość funkcji klaryfikacji**\n",
    " \n",
    "    W obu testach moduł klaryfikacji uznał zapytania za niejednoznaczne, mimo że były one stosunkowo precyzyjne. Model o mniejszej liczbie parametrów taki jak `gemma2:2b` ma tendencję do nadinterpretacji instrukcji. W przyszłych iteracjach należałoby albo dostroić prompt za pomocą few-shot prompting z przykładami pytań jednoznacznych, albo zastosować silniejszy model do tego konkretnego zadania np. model 7B. Mimo to, zachowanie systemu - ostrzeżenie zamiast blokady - było akceptowalne.\n",
    "\n",
    "5. **Podsumowanie**\n",
    "\n",
    "    Advanced RAG zrealizował założenia architektury odpornej na błędy. Połączenie wyszukiwania hybrydowego, analizy intencji oraz weryfikacji faktów pozwoliło uzyskać system, który precyzyjnie odpowiedzia na pytanie z bazy i skutecznie obronił się przed wygenerowaniem fałszywej informacji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ccf189",
   "metadata": {},
   "source": [
    "## ETAP V — benchmark jakości RAG-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8209db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zapytanie: 'Kim był Jan Paweł II?'\n",
      "\n",
      "Zaczynamy! Pytanie użytkownika: 'Kim był Jan Paweł II?'\n",
      "Wykryto niejednoznaczność, ale kontynuuję na potrzeby testu.\n",
      "   -> System zapytałby: Kim był Jan Paweł II w kontekście jego roli jako papież?...\n",
      "Rozpoczynam analizę intencji. Tryb: FACTUAL. Wagi -> ES: 0.8, Qdrant: 0.2\n",
      "Rozpoczynam filtrowanie. Wybrano: 4 dokumentów. Odrzucono krótkie: 0\n",
      "Generowanie odpowiedzi.\n",
      "   -> Wstępny draft: Jan Paweł II był papieżem Kościoła katolickiego....\n",
      "Weryfikacja faktów.\n",
      "Zweryfikowano pomyślnie.\n",
      "--------------------------------------------------\n",
      "Zapytanie: 'Kim jest Howard W. Campbell?'\n",
      "\n",
      "Zaczynamy! Pytanie użytkownika: 'Kim jest Howard W. Campbell?'\n",
      "Wykryto niejednoznaczność, ale kontynuuję na potrzeby testu.\n",
      "   -> System zapytałby: Kim jest Howard W. Campbell jako osoba?...\n",
      "Rozpoczynam analizę intencji. Tryb: SEMANTIC. Wagi -> ES: 0.4, Qdrant: 0.6\n",
      "Rozpoczynam filtrowanie. Wybrano: 4 dokumentów. Odrzucono krótkie: 0\n",
      "Generowanie odpowiedzi.\n",
      "   -> Wstępny draft: Howard W. Campbell, jr., został oskarżony o zbrodniarz wojenny....\n",
      "Weryfikacja faktów.\n",
      "Halucynacja modelu! Odrzucono: W odpowiedzi systemu nie ma żadnych informacji o Renee Alway.  Odpowiedź skupia się jedynie na Howard W. Campbell i jego oskarżaniu o zbrodniarz wojenny.\n",
      "Odłożono pytanie do wyjaśnienia: 'Kim jest Howard W. Campbell?'\n",
      "--------------------------------------------------\n",
      "Zapytanie: 'Co to jest Światowy Dzień Wody?'\n",
      "\n",
      "Zaczynamy! Pytanie użytkownika: 'Co to jest Światowy Dzień Wody?'\n",
      "Wykryto niejednoznaczność, ale kontynuuję na potrzeby testu.\n",
      "   -> System zapytałby: Co to jest Światowy Dzień Wody? - Pytanie o znaczenie wydarzenia...\n",
      "Rozpoczynam analizę intencji. Tryb: SEMANTIC. Wagi -> ES: 0.4, Qdrant: 0.6\n",
      "Rozpoczynam filtrowanie. Wybrano: 4 dokumentów. Odrzucono krótkie: 0\n",
      "Generowanie odpowiedzi.\n",
      "   -> Wstępny draft: Nie wiem....\n",
      "Weryfikacja faktów.\n",
      "Halucynacja modelu! Odrzucono: System nie podał żadnych konkretnych informacji na temat Światowego Dnia Wody. Zawiera jedynie fragmenty z różnych źródeł i nie przedstawia jasnej odpowiedzi.\n",
      "Odłożono pytanie do wyjaśnienia: 'Co to jest Światowy Dzień Wody?'\n",
      "--------------------------------------------------\n",
      "Zapytanie: 'Kim jest Barbara Bilińska?'\n",
      "\n",
      "Zaczynamy! Pytanie użytkownika: 'Kim jest Barbara Bilińska?'\n",
      "Wykryto niejednoznaczność, ale kontynuuję na potrzeby testu.\n",
      "   -> System zapytałby: Kim jest Barbara Bilińska, osoba publiczna?...\n",
      "Rozpoczynam analizę intencji. Tryb: SEMANTIC. Wagi -> ES: 0.4, Qdrant: 0.6\n",
      "Rozpoczynam filtrowanie. Wybrano: 4 dokumentów. Odrzucono krótkie: 0\n",
      "Generowanie odpowiedzi.\n",
      "   -> Wstępny draft: Prof. dr hab. Barbara Bilińska jest członkinią korespondent PAN....\n",
      "Weryfikacja faktów.\n",
      "Zweryfikowano pomyślnie.\n",
      "--------------------------------------------------\n",
      "Zapytanie: 'jak wzrost cen zmienia płace'\n",
      "\n",
      "Zaczynamy! Pytanie użytkownika: 'jak wzrost cen zmienia płace'\n",
      "Wykryto niejednoznaczność, ale kontynuuję na potrzeby testu.\n",
      "   -> System zapytałby: Jak wzrost cen wpływa na średnie płace w poszczególnych sektorach gospodarki?...\n",
      "Rozpoczynam analizę intencji. Tryb: SEMANTIC. Wagi -> ES: 0.4, Qdrant: 0.6\n",
      "Rozpoczynam filtrowanie. Wybrano: 4 dokumentów. Odrzucono krótkie: 0\n",
      "Generowanie odpowiedzi.\n",
      "   -> Wstępny draft: Nie wiem....\n",
      "Weryfikacja faktów.\n",
      "Halucynacja modelu! Odrzucono: System nie dostarcza żadnych informacji na temat wpływu wzrostu cen na płace. W źródłach jedynie informacje o wzroście ruchu organicznego.\n",
      "--------------------------------------------------\n",
      "Zapytanie: 'ponoszenie konsekwencji za swoje błędy'\n",
      "\n",
      "Zaczynamy! Pytanie użytkownika: 'ponoszenie konsekwencji za swoje błędy'\n",
      "Wykryto niejednoznaczność, ale kontynuuję na potrzeby testu.\n",
      "   -> System zapytałby: Wymaga to od osoby, by przejmowała odpowiedzialność za swoje działania i ich skutki, co oznacza, że musi ponosić konsekwencje za błędy....\n",
      "Rozpoczynam analizę intencji. Tryb: SEMANTIC. Wagi -> ES: 0.4, Qdrant: 0.6\n",
      "Rozpoczynam filtrowanie. Wybrano: 4 dokumentów. Odrzucono krótkie: 0\n",
      "Generowanie odpowiedzi.\n",
      "   -> Wstępny draft: Nie wiem....\n",
      "Weryfikacja faktów.\n",
      "Halucynacja modelu! Odrzucono: System nie dostarcza żadnych informacji na temat konsekwencji za błędy. W źródłach brak szczegółowych opisów i przykładów.\n",
      "Odłożono pytanie do wyjaśnienia: 'ponoszenie konsekwencji za swoje błędy'\n",
      "--------------------------------------------------\n",
      "Zapytanie: 'Co mówi PAN o kryzysie?'\n",
      "\n",
      "Zaczynamy! Pytanie użytkownika: 'Co mówi PAN o kryzysie?'\n",
      "Wykryto niejednoznaczność, ale kontynuuję na potrzeby testu.\n",
      "   -> System zapytałby: Czy Pan Piotr Nowak (PAN) ma jakieś konkretne stanowisko na temat kryzysu?...\n",
      "Rozpoczynam analizę intencji. Tryb: FACTUAL. Wagi -> ES: 0.8, Qdrant: 0.2\n",
      "Rozpoczynam filtrowanie. Wybrano: 4 dokumentów. Odrzucono krótkie: 0\n",
      "Generowanie odpowiedzi.\n",
      "   -> Wstępny draft: Nie wiem....\n",
      "Weryfikacja faktów.\n",
      "Halucynacja modelu! Odrzucono: System nie ma żadnych informacji o kryzysie i nie może odpowiedzieć na pytanie. Informacje o kryzysie pochodzą z innych źródeł.\n",
      "--------------------------------------------------\n",
      "Zapytanie: 'najlepsze filmy 2024'\n",
      "\n",
      "Zaczynamy! Pytanie użytkownika: 'najlepsze filmy 2024'\n",
      "Wykryto niejednoznaczność, ale kontynuuję na potrzeby testu.\n",
      "   -> System zapytałby: Filmy roku 2024 (najlepsze)...\n",
      "Rozpoczynam analizę intencji. Tryb: FACTUAL. Wagi -> ES: 0.8, Qdrant: 0.2\n",
      "Rozpoczynam filtrowanie. Wybrano: 4 dokumentów. Odrzucono krótkie: 0\n",
      "Generowanie odpowiedzi.\n",
      "   -> Wstępny draft: Nie wiem....\n",
      "Weryfikacja faktów.\n",
      "Halucynacja modelu! Odrzucono: System nie ma dostępu do informacji o najlepszych filmach w 2024 roku.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from rag_pipeline import run_rag_pipeline\n",
    "import pandas as pd\n",
    "\n",
    "benchmark_data = [\n",
    "    {\n",
    "        \"Zapytanie\": \"Kim był Jan Paweł II?\",\n",
    "        \"Oczekiwane\": \"Oczekiwana poprawna odpowiedź z bazy.\"\n",
    "    },\n",
    "    {\n",
    "        \"Zapytanie\": \"Kim jest Howard W. Campbell?\",\n",
    "        \"Oczekiwane\": \"Oczekiwana poprawna odpowiedź.\"\n",
    "    },\n",
    "    {\n",
    "        \"Zapytanie\": \"Co to jest Światowy Dzień Wody?\",\n",
    "        \"Oczekiwane\": \"Oczekiwana definicja lub data z bazy.\"\n",
    "    },\n",
    "    {\n",
    "        \"Zapytanie\": \"Kim jest Barbara Bilińska?\",\n",
    "        \"Oczekiwane\": \"Oczekiwana informacja o profesurze/PAN.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"Zapytanie\": \"jak wzrost cen zmienia płace\", \n",
    "        \"Oczekiwane\": \"Brak danych - Safe Mode.\"\n",
    "    },\n",
    "    {\n",
    "        \"Zapytanie\": \"ponoszenie konsekwencji za swoje błędy\",\n",
    "        \"Oczekiwane\": \"Brak danych - Safe Mode.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"Zapytanie\": \"Co mówi PAN o kryzysie?\",\n",
    "        \"Oczekiwane\": \"Wykrycie niejednoznaczności.\"\n",
    "    },\n",
    "    {\n",
    "        \"Zapytanie\": \"najlepsze filmy 2024\",\n",
    "        \"Oczekiwane\": \"Brak danych - Safe Mode.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for item in benchmark_data:\n",
    "    print(f\"Zapytanie: '{item['Zapytanie']}'\")\n",
    "    \n",
    "    response = run_rag_pipeline(item['Zapytanie'])\n",
    "    \n",
    "    short_response = response[:100] + \"...\" if len(response) > 100 else response\n",
    "    results_list.append({\n",
    "        \"Zapytanie\": item['Zapytanie'],\n",
    "        \"Odpowiedź\": short_response\n",
    "    })\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf141b",
   "metadata": {},
   "source": [
    "#### Tabela\n",
    "\n",
    "| Zapytanie | Typ | Cytuje źródło? | Halucynacja? | Ocena (0–2) | Komentarz / Uzasadnienie |\n",
    "| :--- | :--- | :---: | :---: | :---: | :--- |\n",
    "| **\"Kim był Jan Paweł II?\"** | Dane z bazy | **TAK** | **NIE** | **2** | System poprawnie zidentyfikował papieża na podstawie bazy. Pełny sukces pipeline'u. |\n",
    "| **\"Kim jest Barbara Bilińska?\"** | Dane z bazy | **TAK** | **NIE** | **2** | Prawidłowa identyfikacja członkini PAN. Walidacja pozytywna. |\n",
    "| **\"Kim jest Howard W. Campbell?\"** | Dane z bazy | **NIE** | **NIE** | **1** | Dokument istnieje w bazie, Generator stworzył poprawny draft, ale Walidator go odrzucił. System zadziałał zbyt ostrożnie. |\n",
    "| **\"Co to jest Światowy Dzień Wody?\"** | Dane z bazy | **NIE** | **NIE** | **2** | Dokument w bazie jest zbyt ogólnikowy (\"istotne wydarzenie\"). System słusznie uznał, że to za mało na definicję i uruchomił Safe Mode. |\n",
    "| **\"jak wzrost cen zmienia płace\"** | Lab 6 | **NIE** | **NIE** | **2** | Brak danych ekonomicznych w korpusie. System odmówił spekulacji. |\n",
    "| **\"ponoszenie konsekwencji...\"** | Lab 7 | **NIE** | **NIE** | **2** | Brak odpowiedniego kontekstu. Prawidłowy Safe Mode. |\n",
    "| **\"Co mówi PAN o kryzysie?\"** | Decomposition | **N/D** | **NIE** | **2** | Poprawna detekcja niejednoznaczności i brak wiedzy o kryzysie. Safe Mode. |\n",
    "| **\"najlepsze filmy 2024\"** | Spoza korpusu | **NIE** | **NIE** | **2** | System oparł się pokusie zmyślania listy filmów. Ocena maksymalna za uczciwość. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c87a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "301f7975",
   "metadata": {},
   "source": [
    "#### Wyniki\n",
    "\n",
    "**Analiza Przypadków**\n",
    "\n",
    "Przeprowadzone testy wykazały, że zaimplementowana architektura priorytetyzuje wiarygodność i bezpieczeństwo danych. Dla znanych faktów precyzja jest wysoka: w przypadkach, gdy kontekst był jednoznaczny czyli Jan Paweł II i Barbara Bilińska, system bezbłędnie przechodził cały proces od wyszukiwania do walidacji. \n",
    "W przypadku zapytania o Howarda Campbella, system weryfikujący odrzucił poprawny draft odpowiedzi. Choć jest to błąd typu false negative, w systemach produkcyjnych jest on preferowany nad false positive czyli przepuszczeniem kłamstwa. Wskazuje to na konieczność użycia większego modelu w roli sędziego w przyszłych iteracjach.\n",
    "Dla wszystkich pytań spoza domeny - ekonomia, filmy, ogólne definicje niedostępne w TOP-4 -  system konsekwentnie uruchamiał procedurę Safe Mode, zapisując zapytania do pamięci zamiast generować halucynacje.\n",
    "\n",
    "Powyższe przypadki dowodzą, że system Advanced RAG skutecznie filtruje nie tylko halucynacje, ale także treści niskiej jakości, co jest pożądaną cechą."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2144959",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
